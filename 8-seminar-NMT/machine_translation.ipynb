{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation  \n",
    "So, one you are looking at this notebook, I think that you already known what NMT task is. \n",
    "Nevertheless, in a few words: we have source text in language $X$ and we need to translate it to target language $Y$. \n",
    "\n",
    "Let's take a look at our imports: we are going to use `PyTorch` package for neural networks and `nltk` for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/egor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from src.data_parser import DataParser\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import NMTDataset\n",
    "from src.models import NMTModel, Encoder, Decoder\n",
    "from src.utils import seed_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the keyword all over the Deep Learning? Sure, reproducibility! Let's seed everything that we can seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can consider our dataset a bit closer. \n",
    "Firstly, read out data and split it by language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parser = DataParser('./data/rus.txt')\n",
    "eng, ru = data_parser.split_by_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-----------------------\nenglish language part contains 380911 samples and 16715 unique tokens\n15 most common tokens: \n. : 316061\ni : 128243\ntom : 112017\nyou : 105434\nto : 87889\n? : 65925\nn't : 63063\nthe : 62120\ndo : 52515\na : 45680\nthat : 42940\nis : 41117\nit : 29710\n's : 28314\ndid : 25455\n-----------------------\n-----------------------\nrussian language part contains 380911 samples and 54486 unique tokens\n15 most common tokens: \n. : 314385\n, : 119915\nя : 94112\nне : 84042\nтом : 80915\n? : 65975\nчто : 53731\nв : 39183\nэто : 38346\nты : 34875\nвы : 25724\nмне : 24019\nна : 22589\nу : 19517\nс : 18881\n-----------------------\n"
    }
   ],
   "source": [
    "def get_statistic(language_data, lang_name):\n",
    "    from collections import Counter\n",
    "    words = []\n",
    "    for sample in language_data:\n",
    "        sample = sample.lower()\n",
    "        tokens = nltk.word_tokenize(sample)\n",
    "        words.extend(tokens)\n",
    "    cntr = Counter(words)\n",
    "    print('-----------------------')\n",
    "    print('{} language part contains {} samples and {} unique tokens'.format(lang_name, len(language_data), len(cntr)))\n",
    "    print('15 most common tokens: ')\n",
    "    most_common = cntr.most_common(15)\n",
    "    for token, freq in most_common:\n",
    "        print('{} : {}'.format(token, freq))\n",
    "    print('-----------------------')\n",
    "\n",
    "get_statistic(eng, 'english')\n",
    "get_statistic(ru, 'russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we see that our target language set contains 54k tokens. It's a bit bigger than we thought. For our \"toy\" dataset vocabulary with 54k tokens could be too big. There are several ways to solve this issue:\n",
    "* use special tokenization techniques, like BPE ([paper](https://arxiv.org/abs/1508.07909), [blogpost](https://leimao.github.io/blog/Byte-Pair-Encoding/), [fastest implementation by VK team](https://github.com/VKCOM/YouTokenToMe))\n",
    "* use special `[UNK]` token to replace subwords with low frequency.\n",
    "\n",
    "On this notebook we are going to use second way. `threshold=0.7` for Russian language tokenizer means that for out vocabulary we are going to use 0.7 * 54k ~ 37k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer.build_vocab(eng, './data/vocab_eng.txt')\n",
    "Tokenizer.build_vocab(ru, './data/vocab_ru.txt', threshold=0.7)\n",
    "\n",
    "eng_tokenizer = Tokenizer('eng', './data/vocab_eng.txt')\n",
    "ru_tokenizer = Tokenizer('ru', './data/vocab_ru.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------- test eng ----------\nI love dogs and cats\n['<BOS>', 'i', 'love', 'dogs', 'and', 'cats', '<EOS>']\n[0, 5, 169, 740, 46, 804, 1]\n['<BOS>', 'i', 'love', 'dogs', 'and', 'cats', '<EOS>']\n---------- test ru ----------\nЯ люблю собак и кошек\n['<BOS>', 'я', 'люблю', 'собак', 'и', 'кошек', '<EOS>']\n[0, 6, 172, 1417, 26, 1575, 1]\n['<BOS>', 'я', 'люблю', 'собак', 'и', 'кошек', '<EOS>']\n"
    }
   ],
   "source": [
    "test_strings = ['I love dogs and cats', 'Я люблю собак и кошек']\n",
    "inform = ['---------- test eng ----------', '---------- test ru ----------']\n",
    "tokenizers = [eng_tokenizer, ru_tokenizer]\n",
    "for test_str, inf, tokenizer in list(zip(test_strings, inform, tokenizers)):\n",
    "    print(inf)\n",
    "    print(test_str)\n",
    "    tokenized = tokenizer.tokenize(test_str)\n",
    "    print(tokenized)\n",
    "    encoded = tokenizer.encode(tokenized)\n",
    "    print(encoded)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few words about model\n",
    "\n",
    "Machine translation is a classic example of seq2seq task and the main architecture for this problem is called `encoder-decoder`.  \n",
    "`Encoder` part is used to project our source text to a latent space.  \n",
    "`Decoder` part gets latent vector from an encoder (for RNN, it could be a last hidden state) to generate a sequence on target language.  \n",
    "After that we train our system like a classic autoregressive Language Model trying to predict next token.\n",
    "For encoder and decoder we can use several NN architectures: RNNs (this notebook), CNNs, Transformers (next class).  \n",
    "\n",
    "You can read [this](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/) blogpost, if you are still missing something.\n",
    "\n",
    "![seq2seq](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset': {\n",
    "        'source_pad_len': 10,\n",
    "        'target_pad_len': 10\n",
    "    },\n",
    "    'dataloader': {\n",
    "        'train_bs': 40,\n",
    "        'test_bs': 40\n",
    "    },\n",
    "    'encoder_cfg': {\n",
    "        'vocab_size': eng_tokenizer.get_vocab_size(),\n",
    "        'embedding_size': 256,\n",
    "        'hidden_size': 128\n",
    "    },\n",
    "    'decoder_cfg': {\n",
    "        'vocab_size': ru_tokenizer.get_vocab_size(),\n",
    "        'embedding_size': 256,\n",
    "        'hidden_size': 128\n",
    "    },\n",
    "    'optim': {\n",
    "        'lr': 5e-5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_parser.train_test_split(0.9)\n",
    "\n",
    "train_dataset = NMTDataset(train, eng_tokenizer, ru_tokenizer, **config['dataset'])\n",
    "test_dataset = NMTDataset(test, eng_tokenizer, ru_tokenizer, **config['dataset'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['dataloader']['train_bs'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config['dataloader']['test_bs'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "Encoder baseline model contains: Embedding -> GRU -> SpatialDropout  \n",
    "Decoder: Embedding -> GRU -> pre-head layer -> head layer (with [weight tying](https://arxiv.org/abs/1608.05859))\n",
    "\n",
    "### How to improve baseline model?\n",
    "* Use `pad_packed_sequence` and `pack_padded_sequence` methods in Encoder and Decoder.\n",
    "* Use attention! In `src/models.py` file you can find class for GlobalAttention. I **STRONGLY RECOMMEND** to read this [article](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html). (**DO NOT FORGET ABOUT ATTENTION MASKS FOR PAD TOKENS!**)\n",
    "* The baseline model is trained by [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) method, you can also read about [Professor forcing](https://arxiv.org/abs/1610.09038).\n",
    "* You can find something interesting [here](https://arxiv.org/abs/1409.3215) and [here](https://arxiv.org/abs/1409.0473).\n",
    "* Implement bidirectional GRU/LSTM in encoder.\n",
    "* You can try to use self-attention in encoder or decoder (**DO NOT FORGET ABOUT ATTENTION MASKS FOR PAD TOKENS!**).\n",
    "* Write validation loop (for example, check loss on validation dataset):)\n",
    "* You can try to implement [beam-search](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)/[nucleus sampling](https://arxiv.org/abs/1904.09751).\n",
    "* Hyper-parameter tuning.\n",
    "* Read about BLEU metric and realize, how you can score it better (see the last cell).\n",
    "\n",
    "You can edit everything you want, your main task is get the highest BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder(**config['encoder_cfg'])\n",
    "decoder = Decoder(**config['decoder_cfg'])\n",
    "\n",
    "model = NMTModel(encoder, decoder).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), config['optim']['lr'])\n",
    "criterion = torch.nn.NLLLoss(ignore_index = ru_tokenizer.encode(['<PAD>'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, test_loader, criterion) -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, criterion, epoch, log_step=200):\n",
    "    model.train()\n",
    "    loss_val = []\n",
    "    avg_loss = []\n",
    "    iter_step = 1\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        preds = model(batch)\n",
    "        preds = preds.permute(0, 2, 1)\n",
    "        loss = criterion(preds, batch['target_for_loss'])\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        if iter_step % log_step == 0:\n",
    "            avg_loss_val = sum(avg_loss) / len(avg_loss)\n",
    "            print('epoch\\t{}\\t[{}/{}]\\tloss: {:4f}'.format(epoch, iter_step, len(loader), avg_loss_val))\n",
    "            avg_loss = []\n",
    "            loss_val.append(avg_loss_val)\n",
    "        iter_step += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "EPOCHS = 2\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = train_epoch(model, optimizer, train_loader, criterion, epoch)\n",
    "    losses.extend(epoch_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sent, device):\n",
    "    model.eval()\n",
    "    sent = eng_tokenizer.encode(eng_tokenizer.tokenize(sent))\n",
    "    dec_sent = model.translate(sent, device)\n",
    "    return ' '.join(ru_tokenizer.decode(dec_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'i love cats', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "translated = []\n",
    "target = []\n",
    "\n",
    "for source, target_ in test:\n",
    "  translated.append(translate(model, source, device))\n",
    "  target.append(target_)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(translated, [target])\n",
    "print(bleu.score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorchenvcondadd1df8879e6648999b744b4f723a1baa",
   "display_name": "Python 3.7.6 64-bit ('pytorch_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}