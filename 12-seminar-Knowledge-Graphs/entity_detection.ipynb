{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Detection\n",
    "The task of entity detection is a basic task for the knowledge graph handling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](knowledge-graph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for data preprocessing\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "\n",
    "from utils import process_original_entity, repalce_punc, processed_text, process_entity\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Let us prepare the data for the futher task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(src_list, pattern_list):\n",
    "    indices = None\n",
    "    for i in range(len(src_list)):\n",
    "        match = 1\n",
    "        for j in range(len(pattern_list)):\n",
    "            if src_list[i + j] != pattern_list[j]:\n",
    "                match = 0\n",
    "                break\n",
    "        if match:\n",
    "            indices = range(i, i + len(pattern_list))\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_ngram(tokens):\n",
    "    ngram = []\n",
    "    for i in range(1, len(tokens) + 1):\n",
    "        for s in range(len(tokens) - i + 1):\n",
    "            ngram.append((\" \".join(tokens[s: s + i]), s, i + s))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_linking(sent, dbpedia_text, original):\n",
    "    tokens = sent.split()\n",
    "    label = [\"O\"] * len(tokens)\n",
    "    exact_match = False\n",
    "\n",
    "    pattern = r'(^|\\s)(%s)($|\\s)' % (re.escape(dbpedia_text))\n",
    "    entity_span = None\n",
    "    if re.search(pattern, sent):\n",
    "        entity_span = get_indices(tokens, dbpedia_text.split())\n",
    "    pattern = r'(^|\\s)(%s)($|\\s)' % (re.escape(original))\n",
    "    if re.search(pattern, sent):\n",
    "        entity_span = get_indices(tokens, original.split())\n",
    "    if entity_span != None:\n",
    "        exact_match = True\n",
    "        for i in entity_span:\n",
    "            label[i] = \"I\"\n",
    "    else:\n",
    "        n_gram_candidate = get_ngram(tokens)\n",
    "        n_gram_candidate = sorted(n_gram_candidate, key=lambda x: fuzz.token_sort_ratio(x[0], dbpedia_text),\n",
    "                                  reverse=True)\n",
    "        top = n_gram_candidate[0]\n",
    "        for i in range(top[1], top[2]):\n",
    "            label[i] = 'I'\n",
    "    entity_text = []\n",
    "    for l, t in zip(label, tokens):\n",
    "        if l == 'I':\n",
    "            entity_text.append(t)\n",
    "    entity_text = \" \".join(entity_text)\n",
    "    label = \" \".join(label)\n",
    "    return entity_text, label, exact_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our datasets for train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what film did peter menzies jr. do cinematography for\" entity = \"Peter_Menzies_Jr.\" processed_query\n",
    "# = processed_text(repalce_punc(question)) processed_candidate = process_entity(repalce_punc(entity))\n",
    "# processed_candidate_original = process_original_entity(repalce_punc(entity)) entity_text, label, exact_match =\n",
    "# reverse_linking(processed_query, processed_candidate, processed_candidate_original) print(\"{}\\t{}\\t{}\\t{\n",
    "# }\\n\".format(question, label, entity_text, str(exact_match))) exit()\n",
    "folds = [\"train\", \"valid\", \"test\"]\n",
    "for fold in folds:\n",
    "    exact_match_counter = 0\n",
    "    total = 0\n",
    "    fin = open(\"data/{}.json\".format(fold), \"rt\", encoding=\"utf-8\")\n",
    "    json_data = json.load(fin)\n",
    "    fout = open(\"data/{}.txt\".format(fold), \"wt\", encoding=\"utf-8\")\n",
    "    for instance in json_data[\"Questions\"]:\n",
    "        total += 1\n",
    "        idx = instance[\"ID\"]\n",
    "        sub = instance[\"Subject\"]\n",
    "        pre = instance[\"PredicateList\"][0][\"Predicate\"]\n",
    "        direction = instance[\"PredicateList\"][0][\"Direction\"]\n",
    "        constraint = instance[\"PredicateList\"][0][\"Constraint\"]\n",
    "        free_pre = instance[\"FreebasePredicate\"]\n",
    "        question = instance[\"Query\"]\n",
    "        entity = sub.replace(\"http://dbpedia.org/resource/\", \"\")\n",
    "        processed_query = processed_text(repalce_punc(question))\n",
    "        processed_candidate = process_entity(repalce_punc(entity))\n",
    "        processed_candidate_original = process_original_entity(repalce_punc(entity))\n",
    "        entity_text, label, exact_match = reverse_linking(processed_query, processed_candidate,\n",
    "                                                          processed_candidate_original)\n",
    "        fout.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(idx, processed_query, sub, pre, direction,\n",
    "                                                             pre + \"@\" + direction + \"@\" + str(constraint),\n",
    "                                                             free_pre, label))  # entity_text, str(exact_match)\n",
    "        if exact_match:\n",
    "            exact_match_counter += 1\n",
    "    print(\"{}\\t{} / {} : {}\".format(fold, exact_match_counter, total, exact_match_counter / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part - training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some methods for evaluation and creating SQdataset\n",
    "from entity_detection.nn.args import get_args\n",
    "from entity_detection.nn.evaluation import evaluation\n",
    "from entity_detection.nn.sq_entity_dataset import SQdataset\n",
    "from entity_detection.nn.entity_detection import EntityDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default configuration in : args.py\n",
    "args = get_args([\"--entity_detection_mode\", \"LSTM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU will be very usefull for this task\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Note: You are using GPU for training\")\n",
    "    device = torch.device(\"gpu\")\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us handle the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data for training\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQdataset with fields: id, sub, entity, relation, obj, text, ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = SQdataset.splits(TEXT, ED, args.data_dir)\n",
    "TEXT.build_vocab(train, dev, test)\n",
    "ED.build_vocab(train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors = torch.Tensor(len(TEXT.vocab), args.words_dim)\n",
    "for i, token in enumerate(TEXT.vocab.itos):\n",
    "    TEXT.vocab.vectors[i] = torch.FloatTensor(args.words_dim).uniform_(-0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an iterator that loads batches of data from a Dataset.\n",
    "train_iter = data.Iterator(train, batch_size=args.batch_size, device=device, train=True, repeat=False,\n",
    "                           sort=False, shuffle=True, sort_within_batch=False)\n",
    "dev_iter = data.Iterator(dev, batch_size=args.batch_size, device=device, train=False, repeat=False,\n",
    "                         sort=False, shuffle=False, sort_within_batch=False)\n",
    "test_iter = data.Iterator(test, batch_size=args.batch_size, device=device, train=False, repeat=False,\n",
    "                          sort=False, shuffle=False, sort_within_batch=False)\n",
    "\n",
    "config = args\n",
    "config.words_num = len(TEXT.vocab)\n",
    "\n",
    "# Our model\n",
    "if args.dataset == 'EntityDetection':\n",
    "    config.label = len(ED.vocab)\n",
    "    model = EntityDetection(config)\n",
    "else:\n",
    "    raise(\"Error Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.embed.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VOCAB num\", len(TEXT.vocab))\n",
    "print(\"Train instance\", len(train))\n",
    "print(\"Dev instance\", len(dev))\n",
    "print(\"Test instance\", len(test))\n",
    "print(\"Entity Type\", len(ED.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let see how model looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it is time to set up training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "early_stop = False\n",
    "best_dev_F = 0\n",
    "best_dev_P = 0\n",
    "best_dev_R = 0\n",
    "iterations = 0\n",
    "iters_not_improved = 0\n",
    "num_dev_in_epoch = (len(train) // args.batch_size // args.dev_every) + 1\n",
    "patience = args.patience * num_dev_in_epoch # for early stopping\n",
    "epoch = 0\n",
    "start = time.time()\n",
    "header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\n",
    "dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{},{}'.split(','))\n",
    "save_path = os.path.join(args.save_path, args.entity_detection_mode.lower())\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print(header)\n",
    "\n",
    "if args.dataset == 'EntityDetection':\n",
    "    index2tag = np.array(ED.vocab.itos)\n",
    "else:\n",
    "    print(\"Wrong Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It will be long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    if early_stop:\n",
    "        print(\"Early Stopping. Epoch: {}, Best Dev F1: {}\".format(epoch, best_dev_F))\n",
    "        break\n",
    "    epoch += 1\n",
    "    train_iter.init_epoch()\n",
    "    n_correct, n_total = 0, 0\n",
    "    n_correct_ed, n_correct_ner , n_correct_rel = 0, 0, 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        # Batch size : (Sentence Length, Batch_size)\n",
    "        iterations += 1\n",
    "        model.train(); optimizer.zero_grad()\n",
    "        scores = model(batch)\n",
    "        # Entity Detection\n",
    "        n_correct += torch.sum((torch.sum((torch.max(scores, 1)[1].view(batch.ed.size()).data == batch.ed.data), \n",
    "                                          dim=0) == batch.ed.size()[0])).item()\n",
    "        loss = criterion(scores, batch.ed.view(-1, 1)[:, 0])\n",
    "\n",
    "        n_total += batch.batch_size\n",
    "        loss.backward()\n",
    "        # clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_gradient)\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate performance on validation set periodically\n",
    "        if iterations % args.dev_every == 0:\n",
    "            model.eval()\n",
    "            dev_iter.init_epoch()\n",
    "            n_dev_correct = 0\n",
    "            n_dev_correct_rel = 0\n",
    "\n",
    "            gold_list = []\n",
    "            pred_list = []\n",
    "\n",
    "            for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                answer = model(dev_batch)\n",
    "\n",
    "                n_dev_correct += ((torch.max(answer, 1)[1].view(dev_batch.ed.size()).data \n",
    "                                   == dev_batch.ed.data).sum(dim=0) == dev_batch.ed.size()[0]).sum()\n",
    "                index_tag = np.transpose(torch.max(answer, 1)[1].view(dev_batch.ed.size()).cpu().data.numpy())\n",
    "                gold_list.append(np.transpose(dev_batch.ed.cpu().data.numpy()))\n",
    "                pred_list.append(index_tag)\n",
    "\n",
    "            P, R, F = evaluation(gold_list, pred_list, index2tag, type=False)\n",
    "            print(\"{} Precision: {:10.6f}% Recall: {:10.6f}% F1 Score: {:10.6f}%\".format(\n",
    "                \"Dev\", 100. * P, 100. * R, 100. * F))\n",
    "\n",
    "            # update model\n",
    "            if F > best_dev_F:\n",
    "                best_dev_F = F\n",
    "                best_dev_P = P\n",
    "                best_dev_R = R\n",
    "                iters_not_improved = 0\n",
    "                snapshot_path = os.path.join(save_path, args.specify_prefix + '_best_model.pt')\n",
    "                # save model, delete previous 'best_snapshot' files\n",
    "                torch.save(model, snapshot_path)\n",
    "            else:\n",
    "                iters_not_improved += 1\n",
    "                if iters_not_improved > patience:\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "        if iterations % args.log_every == 1:\n",
    "            # print progress message\n",
    "            print(log_template.format(time.time() - start,\n",
    "                                          epoch, iterations, 1 + batch_idx, len(train_iter),\n",
    "                                          100. * (1 + batch_idx) / len(train_iter), loss.item(), ' ' * 8,\n",
    "                                          100. * n_correct / n_total, ' ' * 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The evaluation is simple sequence matching in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = np.array(ED.vocab.itos)\n",
    "index2word = np.array(TEXT.vocab.itos)\n",
    "\n",
    "results_path = os.path.join(args.results_path, args.entity_detection_mode.lower())\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(file_name, id_file, output_file):\n",
    "    fin = open(file_name)\n",
    "    fid = open(id_file)\n",
    "    fout = open(output_file, \"w\")\n",
    "\n",
    "    for line, line_id in tqdm(zip(fin.readlines(), fid.readlines())):\n",
    "        query_list = []\n",
    "        query_text = []\n",
    "        line = line.strip().split('\\t')\n",
    "        sent = line[0].strip().split()\n",
    "        pred = line[1].strip().split()\n",
    "        for token, label in zip(sent, pred):\n",
    "            if label == 'I':\n",
    "                query_text.append(token)\n",
    "            if label == 'O':\n",
    "                query_text = list(filter(lambda x: x != '<pad>', query_text))\n",
    "                if len(query_text) != 0:\n",
    "                    query_list.append(\" \".join(list(filter(lambda x:x!='<pad>', query_text))))\n",
    "                    query_text = []\n",
    "        query_text = list(filter(lambda x: x != '<pad>', query_text))\n",
    "        if len(query_text) != 0:\n",
    "            query_list.append(\" \".join(list(filter(lambda x:x!='<pad>', query_text))))\n",
    "            query_text = []\n",
    "        if len(query_list) == 0:\n",
    "            query_list.append(\" \".join(list(filter(lambda x:x!='<pad>',sent))))\n",
    "        fout.write(\" %%%% \".join([line_id.strip()]+query_list)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_iter=test_iter, dataset=test, data_name=\"test\"):\n",
    "    print(\"Dataset: {}\".format(data_name))\n",
    "    model.eval()\n",
    "    dataset_iter.init_epoch()\n",
    "\n",
    "    n_correct = 0\n",
    "    fname = \"{}.txt\".format(data_name)\n",
    "    temp_file = 'tmp'+fname\n",
    "    results_file = open(temp_file, 'w')\n",
    "\n",
    "    gold_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    for data_batch_idx, data_batch in enumerate(dataset_iter):\n",
    "        scores = model(data_batch)\n",
    "        n_correct += torch.sum(torch.sum(torch.max(scores, 1)[1].view(data_batch.ed.size()).data \n",
    "                                         == data_batch.ed.data, dim=1) == data_batch.ed.size()[0]).item()\n",
    "        index_tag = np.transpose(torch.max(scores, 1)[1].view(data_batch.ed.size()).cpu().data.numpy())\n",
    "        tag_array = index2tag[index_tag]\n",
    "        index_question = np.transpose(data_batch.text.cpu().data.numpy())\n",
    "        question_array = index2word[index_question]\n",
    "        gold_list.append(np.transpose(data_batch.ed.cpu().data.numpy()))\n",
    "        gold_array = index2tag[np.transpose(data_batch.ed.cpu().data.numpy())]\n",
    "        pred_list.append(index_tag)\n",
    "        for question, label, gold in zip(question_array, tag_array, gold_array):\n",
    "            results_file.write(\"{}\\t{}\\t{}\\n\".format(\" \".join(question), \" \".join(label), \" \".join(gold)))\n",
    "\n",
    "\n",
    "    P, R, F = evaluation(gold_list, pred_list, index2tag, type=False)\n",
    "    print(\"{} Precision: {:10.6f}% Recall: {:10.6f}% F1 Score: {:10.6f}%\".format(\"Dev\", 100. * P, 100. * R,\n",
    "                                                                                 100. * F))\n",
    "\n",
    "    results_file.flush()\n",
    "    results_file.close()\n",
    "    convert(temp_file, os.path.join(args.data_dir, \"lineids_{}.txt\".format(data_name)), \n",
    "            os.path.join(results_path, \"query.{}\".format(data_name)))\n",
    "    os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on the dev set and write the output to a file\n",
    "predict(dataset_iter=dev_iter, dataset=dev, data_name=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on the test set and write the output to a file\n",
    "predict(dataset_iter=test_iter, dataset=test, data_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is more!\n",
    "Check out the repository with full baselines on this data: https://github.com/castorini/BuboQA \n",
    "This seminar is also based on that repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
